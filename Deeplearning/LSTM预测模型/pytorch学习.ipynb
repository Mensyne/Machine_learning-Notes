{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.0700e-34]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9310, 0.1852, 0.5039],\n",
       "        [0.5701, 0.4723, 0.3937],\n",
       "        [0.5122, 0.4529, 0.4210],\n",
       "        [0.7478, 0.5877, 0.5297],\n",
       "        [0.0749, 0.5573, 0.0579]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4129, 0.0074, 0.5132],\n",
       "        [0.8724, 0.5890, 0.4506],\n",
       "        [0.2324, 0.6200, 0.0672],\n",
       "        [0.8254, 0.6396, 0.6155],\n",
       "        [0.5530, 0.4882, 0.4282]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3439, 0.1926, 1.0172],\n",
       "        [1.4425, 1.0613, 0.8443],\n",
       "        [0.7447, 1.0729, 0.4881],\n",
       "        [1.5732, 1.2273, 1.1452],\n",
       "        [0.6279, 1.0454, 0.4862]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3439, 0.1926, 1.0172],\n",
       "        [1.4425, 1.0613, 0.8443],\n",
       "        [0.7447, 1.0729, 0.4881],\n",
       "        [1.5732, 1.2273, 1.1452],\n",
       "        [0.6279, 1.0454, 0.4862]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3439, 0.1926, 1.0172],\n",
       "        [1.4425, 1.0613, 0.8443],\n",
       "        [0.7447, 1.0729, 0.4881],\n",
       "        [1.5732, 1.2273, 1.1452],\n",
       "        [0.6279, 1.0454, 0.4862]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(5,3)\n",
    "torch.add(x,y,out=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任何改变tensor内容的操作都会在方法名后加一个下划线\"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3439, 0.1926, 1.0172],\n",
       "        [1.4425, 1.0613, 0.8443],\n",
       "        [0.7447, 1.0729, 0.4881],\n",
       "        [1.5732, 1.2273, 1.1452],\n",
       "        [0.6279, 1.0454, 0.4862]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3439, 0.1926, 1.0172],\n",
       "        [1.4425, 1.0613, 0.8443],\n",
       "        [0.7447, 1.0729, 0.4881],\n",
       "        [1.5732, 1.2273, 1.1452],\n",
       "        [0.6279, 1.0454, 0.4862]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1852, 0.4723, 0.4529, 0.5877, 0.5573])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9310, 0.1852, 0.5039],\n",
       "        [0.5701, 0.4723, 0.3937],\n",
       "        [0.5122, 0.4529, 0.4210],\n",
       "        [0.7478, 0.5877, 0.5297],\n",
       "        [0.0749, 0.5573, 0.0579]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch的tensor 和 numpy 的array 共享他们存储空间，修改一个会导致另外一个也被修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将numpy的Array转换成torch的tensor\n",
    "import  numpy as np\n",
    "a = np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(a,1,out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求导运算 requrires_grad = True 用来跟踪该变量相关的计算操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x +2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7fee007c6590>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 每个tensor 都带有属性.grad_fn，该属性引用创建了这个变量的Fucntion\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[27., 27.],\n",
       "         [27., 27.]], grad_fn=<MulBackward0>),\n",
       " tensor(27., grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y*y*3\n",
    "out = z.mean()\n",
    "z,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 来计算梯度  梯度计算需要调用方法.backward(),该变量是一个标量，即仅有一个元素，\n",
    "## 那么不需要要传递任何参数给的方法 .backward(),当包含多个元素时候，那么就必须指定一个\n",
    "## gradient 参数 来匹配尺寸的大小的tensor\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-830.6531, -753.8757,  242.3695], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一般来说 torch.autograd 就是用于计算雅克比向量乘积的工具\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "y = x*2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y*2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1200e+01, 5.1200e+02, 5.1200e-04])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor([0.1,1.0,0.000001],dtype=torch.float)\n",
    "y.backward(v)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "## 加入with torch.no_grad()就可以停止追踪变量历史进行自动梯度计算\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个简单神经网络\n",
    ".用神经对输入进行处理\n",
    "\n",
    ".计算代价值\n",
    "\n",
    ".将梯度传播回神经网络的参数中\n",
    "\n",
    ".更新网络中的权重 weight = weight+ learning_rate*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*5*5,120) # an affine operation: y = Wx + b\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2)) # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2) # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] ## all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[ 1.8910e-01,  1.4928e-01,  6.2433e-02,  7.6362e-03, -1.3160e-01],\n",
       "           [-1.6989e-01,  1.0392e-01,  9.8101e-03, -9.0725e-02, -3.8924e-02],\n",
       "           [-1.1899e-01, -1.8024e-01, -1.6835e-01,  8.7790e-02, -1.2563e-01],\n",
       "           [-1.6018e-01,  1.7934e-01, -6.3487e-02, -2.5271e-02,  1.0180e-04],\n",
       "           [-1.5666e-02,  1.3138e-01,  1.4238e-01,  1.4650e-01, -1.5941e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.8333e-01,  5.8387e-02,  7.4210e-02,  3.1307e-04,  3.2712e-02],\n",
       "           [ 8.9550e-02, -1.5790e-01,  1.9824e-01, -1.6551e-02,  8.3199e-02],\n",
       "           [-1.7080e-01, -1.5473e-01,  8.1232e-02,  9.8257e-02, -1.1718e-01],\n",
       "           [ 1.3240e-01,  1.9457e-01, -1.0972e-01,  1.9739e-01,  3.7865e-02],\n",
       "           [-3.3993e-02, -8.4689e-02,  1.7730e-01, -2.9686e-04, -3.8559e-02]]],\n",
       " \n",
       " \n",
       "         [[[-9.9288e-02,  6.9548e-02, -1.5976e-01,  1.4247e-01, -1.7339e-01],\n",
       "           [-1.3801e-01, -1.3348e-01,  3.1307e-02,  7.5065e-02,  1.8500e-01],\n",
       "           [-1.3560e-01, -7.3006e-02,  6.5400e-02,  8.8171e-02,  1.2612e-01],\n",
       "           [-3.4302e-02, -2.6617e-03, -1.3770e-01, -1.0645e-01, -1.6672e-01],\n",
       "           [-1.0181e-02, -8.4908e-02,  1.4030e-02,  6.8083e-02,  1.9925e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0368e-02,  1.5861e-01, -1.7086e-01, -3.8845e-02, -1.5070e-01],\n",
       "           [ 1.2705e-01, -1.8019e-01,  4.1282e-02,  1.9154e-01, -8.2522e-02],\n",
       "           [ 8.6914e-02,  1.8105e-02, -6.6871e-03, -7.5022e-03,  1.9722e-01],\n",
       "           [-1.2027e-01,  7.9128e-02, -7.8251e-02,  8.5976e-02,  1.2719e-01],\n",
       "           [-1.6941e-01, -1.5529e-01,  6.8891e-02, -8.6728e-02,  1.7056e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.4303e-01, -1.2155e-01, -6.9308e-02,  1.3348e-01, -5.2773e-02],\n",
       "           [ 1.9527e-01, -3.4092e-02,  1.8568e-01,  7.7707e-02, -1.3450e-01],\n",
       "           [ 2.8162e-02,  6.5140e-02, -8.9854e-03, -1.7240e-01,  1.0099e-01],\n",
       "           [-1.1443e-01,  1.0159e-01,  3.7311e-02,  2.2589e-02,  5.2375e-02],\n",
       "           [ 9.4510e-03,  1.7542e-01, -1.3045e-01, -1.7409e-01, -1.3198e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.4740e-01,  4.6959e-03,  1.0207e-01,  1.7092e-01,  1.4141e-01],\n",
       "           [-1.1633e-01,  1.0390e-01,  1.9646e-02, -3.2921e-02, -6.2269e-02],\n",
       "           [ 1.6883e-01, -6.8516e-02,  1.3526e-01,  2.9370e-02,  1.4856e-01],\n",
       "           [ 1.7481e-01,  6.1691e-02, -1.3119e-01,  1.8067e-01,  1.4915e-01],\n",
       "           [-6.6543e-02,  9.9695e-02, -5.1822e-02, -1.5328e-01,  1.0076e-01]]]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1367,  0.1794, -0.1516,  0.0409, -0.1608,  0.1745],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 6.8808e-02,  2.3513e-02, -4.9339e-03,  2.5265e-02,  4.4138e-02],\n",
       "           [-7.9600e-02, -1.0566e-04, -5.6010e-02,  3.5645e-02,  2.0112e-02],\n",
       "           [ 1.1151e-02, -6.8394e-02,  5.5501e-02, -6.5268e-02,  1.2626e-02],\n",
       "           [ 6.2586e-02, -5.1665e-03,  3.7501e-02,  2.2620e-02,  2.1408e-02],\n",
       "           [ 1.9295e-02, -1.9637e-02,  1.0653e-02,  3.6517e-02,  4.1058e-02]],\n",
       " \n",
       "          [[-7.6015e-02, -5.9675e-02,  5.8331e-02, -3.4646e-03, -5.0702e-02],\n",
       "           [ 7.9029e-02,  5.4292e-02, -6.4614e-02,  4.2939e-02, -5.1639e-02],\n",
       "           [ 5.1665e-02,  7.8120e-02,  1.5979e-02, -4.3370e-02,  2.4893e-02],\n",
       "           [-3.8953e-03, -2.8439e-02,  2.5741e-02, -5.8553e-02,  2.8257e-02],\n",
       "           [-5.3018e-03,  8.3520e-03,  2.1397e-02,  5.7269e-02,  4.8632e-02]],\n",
       " \n",
       "          [[-2.9397e-02,  3.0683e-02,  3.9904e-02,  4.0656e-02, -5.2981e-02],\n",
       "           [-5.6221e-02,  5.7945e-02,  5.5057e-02,  5.5725e-02, -2.8532e-02],\n",
       "           [-5.8886e-02,  5.0725e-02,  5.5925e-02,  7.7561e-03, -5.1189e-02],\n",
       "           [-6.1274e-02,  5.1481e-02, -8.0969e-02,  6.6494e-02, -5.7052e-02],\n",
       "           [ 9.7778e-03,  7.8122e-02, -1.3756e-02, -2.9723e-02, -6.2970e-02]],\n",
       " \n",
       "          [[ 5.3667e-02, -4.4427e-02, -4.4416e-02,  1.6866e-02,  4.5355e-02],\n",
       "           [ 3.6818e-02,  3.2501e-02,  5.8548e-02,  5.7241e-02,  5.6601e-02],\n",
       "           [-2.9186e-02,  5.4164e-02,  3.2518e-02,  5.9674e-02,  4.6517e-02],\n",
       "           [ 1.5327e-02, -8.3180e-03,  2.4363e-02, -2.9510e-02, -2.7540e-03],\n",
       "           [ 2.7502e-02, -5.2331e-02,  4.8887e-03, -1.0951e-02, -4.0702e-02]],\n",
       " \n",
       "          [[ 4.2095e-02,  7.0668e-02,  5.5938e-02,  7.5346e-02, -1.4037e-02],\n",
       "           [ 7.7181e-02,  4.8133e-02, -1.2887e-02, -9.7945e-03,  7.2125e-02],\n",
       "           [ 1.1250e-05,  8.0881e-02,  3.3695e-02, -1.6912e-03,  6.4348e-02],\n",
       "           [-5.9904e-02,  2.3055e-02, -6.5010e-02,  1.0724e-02,  4.0129e-02],\n",
       "           [-3.6200e-03, -8.2189e-03,  7.9147e-02,  4.2063e-02,  2.4174e-02]],\n",
       " \n",
       "          [[ 7.1556e-02, -6.2164e-02,  4.1878e-02, -3.5721e-02, -2.6418e-02],\n",
       "           [ 5.2203e-02, -4.3384e-02, -6.5248e-02,  2.8441e-02, -7.8529e-02],\n",
       "           [ 6.1601e-02, -3.8544e-02,  5.8279e-02,  7.5255e-02, -6.1496e-02],\n",
       "           [ 4.6247e-02, -3.3568e-02, -3.2739e-02, -7.1857e-02, -2.5414e-02],\n",
       "           [-3.1733e-02, -5.2956e-02,  5.2068e-02,  4.4821e-03,  3.2903e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.4827e-02, -6.4981e-02, -4.7743e-02, -2.3837e-02,  2.3586e-02],\n",
       "           [-6.7919e-02,  7.1317e-02,  9.4199e-03,  5.2501e-02,  6.3113e-02],\n",
       "           [-7.8088e-02,  1.5750e-02,  6.7149e-02,  4.2780e-02,  5.1835e-02],\n",
       "           [ 7.4364e-02,  1.1264e-03,  2.8509e-02, -4.5682e-02,  8.1606e-02],\n",
       "           [-4.0183e-02,  2.7985e-02, -2.5394e-02, -6.4853e-02,  1.1545e-02]],\n",
       " \n",
       "          [[-5.8021e-02, -7.5835e-02,  4.4154e-02, -6.5027e-03,  3.5146e-02],\n",
       "           [ 5.6992e-02, -7.4665e-02, -5.1074e-02, -3.7719e-03,  2.3505e-03],\n",
       "           [ 5.8056e-02,  3.6727e-02,  5.5351e-02,  5.5901e-02, -3.1514e-04],\n",
       "           [ 3.1570e-02, -1.3583e-02,  8.1648e-02, -6.5211e-02, -3.8394e-02],\n",
       "           [ 1.2975e-02,  2.7858e-02,  6.2204e-02, -1.1591e-02, -1.5392e-02]],\n",
       " \n",
       "          [[ 2.1995e-02, -4.9098e-03,  1.6819e-02,  9.7629e-03,  6.6281e-02],\n",
       "           [ 5.6875e-02,  2.4275e-02, -1.6029e-02, -6.6403e-02, -4.4533e-02],\n",
       "           [-5.4934e-03,  4.1944e-02,  6.2480e-02,  2.9199e-02, -4.7409e-02],\n",
       "           [ 5.9176e-02,  7.9122e-02,  2.1323e-02,  6.2478e-02, -5.9279e-02],\n",
       "           [ 3.6395e-02, -4.7118e-02, -6.6916e-02,  2.0316e-02,  3.7597e-02]],\n",
       " \n",
       "          [[ 7.6200e-02,  5.0304e-02, -7.6360e-02, -5.4473e-02,  4.6182e-02],\n",
       "           [-1.7968e-02,  5.4362e-02, -6.5469e-02, -3.7408e-02, -1.0864e-02],\n",
       "           [ 5.3503e-02,  3.0557e-02,  4.6101e-02,  7.2180e-02, -6.6629e-02],\n",
       "           [ 1.1164e-02, -7.9211e-02,  2.6125e-04, -5.1457e-02,  6.9099e-02],\n",
       "           [ 8.1259e-02, -6.8251e-02, -5.0114e-02, -3.0231e-02, -9.9219e-03]],\n",
       " \n",
       "          [[ 3.2218e-02, -1.8537e-02, -4.4875e-02, -3.7687e-02,  6.6299e-02],\n",
       "           [-5.5230e-02,  1.0356e-02, -3.3968e-02,  7.4583e-02, -6.6735e-02],\n",
       "           [-7.9507e-02, -3.0143e-02,  1.1673e-02,  6.3580e-02, -8.5370e-03],\n",
       "           [ 5.7339e-02,  7.5810e-02, -4.7791e-02,  2.6595e-02,  6.9495e-02],\n",
       "           [-6.0881e-02,  2.7553e-02, -6.8423e-02, -6.5224e-02, -5.8320e-02]],\n",
       " \n",
       "          [[-2.7128e-03, -7.6414e-02, -5.6958e-02, -4.3379e-02,  5.2826e-02],\n",
       "           [ 4.4470e-02, -5.2958e-02,  4.8672e-02, -2.4678e-03,  2.7423e-02],\n",
       "           [-6.9356e-02,  7.2497e-02,  7.3911e-02,  4.9082e-02, -5.3812e-02],\n",
       "           [ 6.2663e-02, -3.5974e-02, -5.8518e-02,  6.7654e-02,  5.6599e-02],\n",
       "           [-7.0168e-02,  7.9854e-02, -3.1363e-02, -6.4427e-02,  1.1157e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.8157e-02, -7.3793e-02, -5.4511e-02,  3.1692e-02,  6.7080e-02],\n",
       "           [-1.9791e-02, -3.3712e-02, -3.9746e-02, -5.5336e-02, -1.8145e-02],\n",
       "           [-3.8377e-02, -5.9561e-02, -6.4404e-02, -5.7112e-02,  3.4326e-02],\n",
       "           [ 3.5014e-02, -8.0183e-02,  7.9229e-02,  3.4907e-03, -6.8044e-02],\n",
       "           [-5.3711e-02,  3.9840e-02, -1.9147e-02,  5.3269e-03, -7.4154e-02]],\n",
       " \n",
       "          [[-4.9659e-02, -2.6164e-02,  2.9247e-02, -7.3945e-02,  3.3224e-02],\n",
       "           [ 6.3181e-02,  2.8026e-02,  1.9550e-04, -3.8535e-02, -2.8615e-02],\n",
       "           [-3.5709e-02, -2.4002e-02, -2.5847e-02, -7.3765e-02,  7.4294e-02],\n",
       "           [-6.8881e-02,  5.3652e-02,  5.4602e-02, -3.8299e-03,  3.8605e-02],\n",
       "           [ 5.4550e-02,  1.0329e-02,  7.9390e-02, -2.1816e-02,  7.9982e-02]],\n",
       " \n",
       "          [[ 5.3985e-02,  4.8343e-02,  2.8629e-02,  4.2078e-02,  2.9624e-02],\n",
       "           [ 3.5914e-02, -7.1004e-02, -7.6724e-02, -3.4693e-02,  2.8525e-02],\n",
       "           [ 3.8791e-02, -1.1312e-02,  6.9560e-03,  1.8485e-02,  2.3255e-02],\n",
       "           [-6.3056e-02,  2.7908e-02, -5.6147e-02,  4.3285e-02, -3.3487e-02],\n",
       "           [-2.0378e-02,  1.2448e-02,  6.7250e-03,  4.1035e-02, -2.0757e-04]],\n",
       " \n",
       "          [[ 1.4504e-03,  1.6162e-02, -6.4205e-02, -1.5666e-02,  7.5299e-02],\n",
       "           [ 8.0267e-02, -1.9224e-02, -5.1596e-02,  5.4815e-02,  5.0166e-02],\n",
       "           [ 5.1773e-02,  2.7353e-02, -5.6513e-02,  2.5839e-02,  2.7399e-02],\n",
       "           [-5.0184e-02,  6.8196e-02,  5.1465e-02, -6.7772e-02,  2.3083e-02],\n",
       "           [-5.2970e-02, -1.3190e-02,  6.0294e-02, -4.1542e-02, -7.6397e-02]],\n",
       " \n",
       "          [[ 2.3758e-02, -6.8544e-03,  5.1090e-02,  3.6691e-04,  1.5721e-02],\n",
       "           [-7.1719e-02, -3.7144e-02, -3.3946e-02, -1.9006e-02, -1.9684e-02],\n",
       "           [ 5.4316e-02,  8.4198e-03, -2.7529e-02, -1.2520e-02, -1.8652e-02],\n",
       "           [ 4.1038e-02, -5.3759e-02,  8.1389e-02, -4.8894e-02,  5.2559e-03],\n",
       "           [-8.5721e-03,  2.4573e-02, -3.2890e-02,  4.9451e-02,  5.7735e-02]],\n",
       " \n",
       "          [[-1.4922e-02,  6.6406e-02, -3.2299e-02, -4.1803e-02, -1.2804e-02],\n",
       "           [ 4.6266e-02, -3.1755e-02, -5.4928e-02, -2.3971e-02, -4.2332e-02],\n",
       "           [-2.7061e-02,  5.3256e-02, -6.6681e-02, -2.3180e-02, -7.7184e-02],\n",
       "           [ 2.5196e-02,  2.9586e-03,  1.8039e-02, -6.9659e-02, -6.9006e-02],\n",
       "           [-3.4624e-02,  5.8753e-02, -6.2931e-02,  2.7161e-02, -7.1032e-03]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-4.3064e-02,  7.9652e-02, -1.8934e-02, -5.0787e-02, -8.0239e-02],\n",
       "           [-3.6541e-02,  2.8386e-02,  1.8276e-02,  8.1965e-03,  2.8079e-02],\n",
       "           [-3.2445e-02,  5.3128e-02, -7.7926e-02, -6.1538e-02,  5.9713e-02],\n",
       "           [-4.9833e-02, -1.3821e-02,  3.0762e-02,  5.0187e-03, -1.8798e-02],\n",
       "           [-8.0381e-02, -1.3686e-02,  8.0024e-02, -7.9501e-02,  2.4057e-02]],\n",
       " \n",
       "          [[-6.9194e-03, -5.2140e-02, -4.4459e-02,  5.3919e-02, -6.4315e-02],\n",
       "           [-7.9080e-02, -7.8155e-03, -7.1510e-02,  3.3758e-02,  1.3168e-03],\n",
       "           [-6.1118e-02, -3.4666e-02, -2.3553e-02, -3.0976e-02,  5.9269e-03],\n",
       "           [ 7.8321e-02, -2.7096e-02,  7.1998e-03,  9.6747e-03, -1.3229e-02],\n",
       "           [ 1.6731e-02,  5.3742e-02,  7.3119e-02,  2.6702e-02, -6.9925e-02]],\n",
       " \n",
       "          [[-3.2092e-02,  1.0417e-02, -6.8780e-02, -5.0377e-02,  6.3290e-02],\n",
       "           [-2.8814e-02,  7.5638e-03, -6.5343e-02, -1.2232e-02, -6.3238e-02],\n",
       "           [-2.6292e-02,  7.1694e-02,  5.0244e-02, -4.9546e-02, -2.8364e-02],\n",
       "           [-1.2481e-02, -6.2203e-03, -6.9307e-03, -1.6394e-02, -7.2879e-02],\n",
       "           [-4.1223e-02,  7.7267e-02,  1.5259e-02, -3.5081e-02, -4.3155e-02]],\n",
       " \n",
       "          [[ 3.5503e-02, -1.5267e-02, -3.1104e-02,  2.6313e-02,  3.4615e-02],\n",
       "           [-2.7860e-02,  7.2984e-02, -5.1518e-03, -7.4473e-02,  8.6074e-04],\n",
       "           [ 5.9928e-02, -1.2535e-02,  6.9759e-02, -7.9244e-04,  4.4813e-02],\n",
       "           [-4.2576e-02, -2.6263e-02, -7.5550e-03, -7.0444e-02, -2.2053e-02],\n",
       "           [-3.3064e-02, -1.5870e-02,  3.6893e-02, -4.8292e-02, -3.3218e-03]],\n",
       " \n",
       "          [[ 6.8971e-02, -5.2818e-02,  8.1051e-03,  6.8373e-02,  2.7736e-02],\n",
       "           [-7.7892e-02, -3.1748e-02, -2.1975e-02, -7.0829e-02,  5.2674e-02],\n",
       "           [-3.2295e-02, -6.5577e-02, -2.2468e-02,  3.5369e-02,  4.8447e-02],\n",
       "           [-2.0119e-02,  2.6142e-02, -5.1329e-02, -7.2905e-03, -7.7130e-02],\n",
       "           [-5.7116e-03,  3.5404e-02, -7.5491e-02, -2.7291e-02,  2.3344e-02]],\n",
       " \n",
       "          [[-5.0285e-03, -8.1368e-02, -2.9240e-02, -5.1569e-02,  4.3688e-02],\n",
       "           [ 3.7266e-03,  5.5533e-02,  9.2277e-03, -7.1110e-02,  6.6403e-02],\n",
       "           [ 6.4717e-02,  4.3935e-02,  2.0175e-03, -9.9759e-03,  7.4233e-02],\n",
       "           [ 6.8473e-02, -1.7774e-02,  3.8485e-02, -5.3710e-02,  6.3175e-02],\n",
       "           [ 7.5231e-02, -1.1616e-02, -2.8083e-02, -3.5061e-02,  6.4287e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 4.3447e-02, -3.9043e-02,  7.9877e-02,  1.6863e-02, -1.1150e-02],\n",
       "           [-7.5976e-02,  2.0801e-02, -2.5230e-02,  1.6077e-02,  5.6802e-03],\n",
       "           [-7.4042e-02,  6.7380e-02, -7.6292e-02,  7.8135e-02, -4.9758e-03],\n",
       "           [-6.8410e-02, -2.6234e-02, -6.5228e-02, -4.6208e-02,  1.2002e-02],\n",
       "           [ 6.1600e-02,  5.7099e-02,  1.4865e-02, -5.6774e-02,  5.5214e-02]],\n",
       " \n",
       "          [[-6.5114e-02,  8.9741e-03, -4.9298e-02, -2.1832e-02,  6.4111e-02],\n",
       "           [-7.9046e-02, -6.5944e-02,  7.3204e-02,  2.3160e-02,  6.3424e-02],\n",
       "           [ 4.0911e-02, -3.7218e-04, -4.3299e-02, -2.3103e-02, -8.0756e-02],\n",
       "           [ 8.1153e-02,  3.8781e-02, -2.7886e-02,  2.4836e-02, -2.7036e-02],\n",
       "           [ 3.0667e-02,  5.6659e-02, -7.5530e-02, -1.2085e-02,  1.7389e-02]],\n",
       " \n",
       "          [[-2.2451e-02,  7.9174e-02,  5.6110e-02,  5.2486e-02,  2.2036e-03],\n",
       "           [-7.9243e-03, -4.0214e-02, -2.4914e-02, -2.8617e-02, -4.1718e-02],\n",
       "           [ 1.3532e-02,  1.2636e-02,  4.4177e-02, -3.8586e-02,  7.9198e-02],\n",
       "           [ 1.3566e-02,  3.6473e-02,  6.7703e-03,  3.0995e-03,  1.9055e-02],\n",
       "           [ 6.7850e-02,  5.1668e-02,  6.6825e-02,  7.5845e-02, -5.1330e-03]],\n",
       " \n",
       "          [[-4.9785e-02, -4.3091e-02, -7.1472e-02,  1.4755e-02,  1.7403e-02],\n",
       "           [-1.3800e-02,  4.4423e-02,  9.6840e-03,  2.1186e-02,  4.5965e-02],\n",
       "           [ 1.4023e-02, -2.3600e-02,  6.4843e-02, -6.6442e-02, -5.5606e-02],\n",
       "           [ 5.0819e-02,  4.9373e-02, -2.1090e-04, -4.2757e-02, -1.9470e-02],\n",
       "           [ 4.7362e-02,  4.9068e-03, -3.3876e-02, -7.7581e-02,  9.9041e-05]],\n",
       " \n",
       "          [[-1.7638e-03, -8.1338e-02,  6.4466e-02, -2.3096e-02,  7.2544e-02],\n",
       "           [-4.1737e-02, -2.8417e-02, -2.4883e-02, -8.8996e-03,  3.8715e-02],\n",
       "           [ 5.7400e-03, -5.7561e-02, -7.5829e-03,  3.4514e-02,  4.8553e-02],\n",
       "           [ 7.7985e-02, -6.8097e-02,  1.4870e-02, -7.1439e-03, -3.0017e-02],\n",
       "           [-6.4688e-02,  1.1193e-02, -6.4632e-02, -3.0375e-02, -5.5471e-02]],\n",
       " \n",
       "          [[ 6.0031e-02,  6.7936e-02,  3.0625e-02,  7.9796e-02, -3.4654e-02],\n",
       "           [ 4.9120e-02, -5.8968e-02,  6.0949e-02,  6.4208e-02,  3.9193e-02],\n",
       "           [ 4.9147e-02, -2.2085e-02, -7.6238e-02,  5.2100e-02, -6.4245e-02],\n",
       "           [-4.6335e-02,  1.3268e-02,  1.5559e-02,  2.6478e-02, -2.7913e-02],\n",
       "           [-6.9753e-03,  3.5329e-02, -4.6773e-02,  5.1517e-02,  1.7831e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.0596e-02,  5.5813e-02,  2.1625e-02, -3.7096e-02, -4.1235e-02],\n",
       "           [ 1.7136e-02, -4.7913e-02,  4.6839e-02, -6.0479e-02,  2.1295e-02],\n",
       "           [-7.8813e-02, -6.4303e-02, -4.7418e-02,  9.3764e-03,  7.5179e-02],\n",
       "           [ 3.8346e-02,  1.7635e-02, -6.6010e-02,  7.7116e-02,  1.4485e-02],\n",
       "           [ 4.3834e-02,  2.9338e-02, -7.0164e-02, -3.5267e-02, -5.5158e-02]],\n",
       " \n",
       "          [[-3.5660e-02,  4.0759e-02, -2.4770e-02, -4.3819e-02,  5.3611e-02],\n",
       "           [ 7.0391e-02,  6.7904e-02,  2.8791e-02, -2.0539e-02,  4.2749e-02],\n",
       "           [-5.0821e-02,  1.1858e-02, -2.1668e-02, -8.1166e-02,  5.4157e-02],\n",
       "           [ 3.4421e-02, -4.4464e-02, -2.9379e-03, -4.8301e-02, -5.5518e-02],\n",
       "           [ 8.0371e-02,  8.1271e-02, -5.6655e-03,  1.6236e-02,  6.1513e-02]],\n",
       " \n",
       "          [[-2.4286e-02, -1.6094e-02, -4.6043e-03,  1.6914e-02, -1.2879e-02],\n",
       "           [ 6.3848e-02, -5.9394e-02,  5.7235e-02,  4.3846e-02,  7.1568e-02],\n",
       "           [-3.8810e-03,  4.7580e-02,  3.2110e-02,  2.7844e-02,  5.1519e-02],\n",
       "           [-5.1623e-02, -6.6242e-02,  1.7546e-02, -1.0430e-02,  1.5082e-02],\n",
       "           [-7.7926e-03, -3.2677e-02,  5.5117e-02,  6.7267e-02,  2.2875e-02]],\n",
       " \n",
       "          [[-6.6187e-02,  4.8106e-02, -3.4183e-02,  5.4562e-02,  1.5492e-02],\n",
       "           [ 8.8819e-03, -1.5372e-02,  1.5197e-02, -1.0973e-02, -5.7422e-02],\n",
       "           [-1.9058e-02, -1.6217e-02,  2.3900e-02, -6.8315e-02, -9.3673e-03],\n",
       "           [-3.4778e-02, -6.3583e-02, -7.2167e-02, -4.9740e-02, -5.1667e-03],\n",
       "           [ 8.1656e-03, -8.6020e-03,  1.0755e-02,  7.9169e-02, -4.7790e-03]],\n",
       " \n",
       "          [[ 7.2322e-02, -4.0876e-03,  6.4546e-02, -1.0433e-02,  8.1294e-02],\n",
       "           [-6.1881e-02, -6.4447e-04, -4.6078e-02,  6.9733e-02, -7.4768e-02],\n",
       "           [-5.5787e-02, -1.7040e-02, -1.5474e-02,  1.0988e-02, -7.7976e-02],\n",
       "           [-8.7920e-03, -1.5157e-02, -5.5416e-02,  2.7255e-03,  2.2769e-02],\n",
       "           [ 1.3230e-02, -2.4308e-02, -3.7642e-02, -1.1579e-02,  3.8169e-02]],\n",
       " \n",
       "          [[ 5.6847e-02,  5.4381e-02, -4.5475e-02, -5.6755e-02, -4.4549e-02],\n",
       "           [ 9.6645e-03, -5.9660e-02, -6.0742e-02, -4.3880e-02, -7.8345e-02],\n",
       "           [-3.4702e-02, -1.3415e-02,  3.8675e-02, -6.2210e-02,  7.8551e-02],\n",
       "           [ 5.7058e-02, -2.1403e-02,  2.0809e-02,  7.2068e-02,  7.6258e-02],\n",
       "           [-6.7123e-02, -8.0822e-03, -6.1838e-02, -3.7913e-02, -2.3997e-02]]]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0604,  0.0005,  0.0250, -0.0806,  0.0515, -0.0224,  0.0370,  0.0453,\n",
       "         -0.0101, -0.0777, -0.0090,  0.0675, -0.0802,  0.0613, -0.0036, -0.0524],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0161, -0.0372, -0.0411,  ...,  0.0239,  0.0183, -0.0212],\n",
       "         [ 0.0033,  0.0482,  0.0114,  ...,  0.0351,  0.0170,  0.0490],\n",
       "         [ 0.0237, -0.0324, -0.0059,  ...,  0.0230, -0.0418,  0.0394],\n",
       "         ...,\n",
       "         [-0.0064,  0.0114, -0.0091,  ...,  0.0252,  0.0326,  0.0328],\n",
       "         [-0.0263,  0.0017, -0.0063,  ...,  0.0378,  0.0022,  0.0439],\n",
       "         [-0.0003, -0.0308, -0.0108,  ...,  0.0216, -0.0484, -0.0312]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0008, -0.0210, -0.0320, -0.0236,  0.0024,  0.0133, -0.0303,  0.0065,\n",
       "         -0.0220, -0.0432, -0.0360,  0.0198, -0.0047,  0.0196,  0.0489,  0.0250,\n",
       "          0.0134, -0.0431,  0.0039, -0.0313, -0.0356, -0.0073, -0.0008,  0.0423,\n",
       "          0.0071,  0.0009, -0.0274,  0.0193, -0.0291,  0.0014, -0.0061, -0.0013,\n",
       "          0.0488,  0.0265, -0.0263,  0.0370, -0.0120, -0.0432,  0.0267,  0.0429,\n",
       "         -0.0407,  0.0438, -0.0432, -0.0005,  0.0439, -0.0232, -0.0371, -0.0064,\n",
       "          0.0396, -0.0334,  0.0450, -0.0338, -0.0350, -0.0252,  0.0461, -0.0391,\n",
       "          0.0193,  0.0338,  0.0119, -0.0271,  0.0293,  0.0207, -0.0127,  0.0211,\n",
       "         -0.0482, -0.0129,  0.0172, -0.0317,  0.0467, -0.0345, -0.0118,  0.0314,\n",
       "         -0.0181, -0.0174,  0.0419, -0.0167, -0.0067, -0.0043,  0.0298, -0.0472,\n",
       "          0.0480,  0.0194,  0.0351,  0.0474,  0.0045,  0.0264,  0.0060,  0.0230,\n",
       "          0.0416,  0.0457,  0.0498, -0.0085,  0.0091, -0.0110,  0.0127,  0.0228,\n",
       "         -0.0290, -0.0472, -0.0040, -0.0238, -0.0227, -0.0203,  0.0130, -0.0454,\n",
       "          0.0083, -0.0464,  0.0437, -0.0088, -0.0440, -0.0454, -0.0149, -0.0097,\n",
       "         -0.0012,  0.0398,  0.0225, -0.0287,  0.0345, -0.0014,  0.0146, -0.0061],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0246, -0.0532, -0.0508,  ..., -0.0893, -0.0589, -0.0243],\n",
       "         [-0.0815, -0.0860, -0.0120,  ..., -0.0074,  0.0667, -0.0489],\n",
       "         [-0.0204, -0.0145, -0.0784,  ..., -0.0574,  0.0768, -0.0826],\n",
       "         ...,\n",
       "         [ 0.0368, -0.0145,  0.0196,  ...,  0.0699, -0.0182, -0.0909],\n",
       "         [-0.0753,  0.0448,  0.0780,  ..., -0.0396, -0.0663,  0.0462],\n",
       "         [ 0.0253, -0.0663, -0.0466,  ...,  0.0613, -0.0676,  0.0423]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0411,  0.0301,  0.0592,  0.0536,  0.0315,  0.0704,  0.0342,  0.0838,\n",
       "          0.0231,  0.0805, -0.0362,  0.0429,  0.0195,  0.0425,  0.0219,  0.0122,\n",
       "          0.0128,  0.0142, -0.0115,  0.0607, -0.0099,  0.0417, -0.0701, -0.0580,\n",
       "          0.0055,  0.0014,  0.0882, -0.0037, -0.0102, -0.0155,  0.0037, -0.0301,\n",
       "          0.0159, -0.0611, -0.0389, -0.0445, -0.0526,  0.0186,  0.0570, -0.0436,\n",
       "         -0.0468,  0.0899, -0.0852,  0.0085,  0.0075, -0.0076,  0.0013,  0.0269,\n",
       "          0.0380, -0.0899, -0.0718,  0.0543, -0.0145, -0.0097, -0.0456, -0.0777,\n",
       "         -0.0699,  0.0443,  0.0190, -0.0882, -0.0586, -0.0066,  0.0081, -0.0409,\n",
       "         -0.0866, -0.0496, -0.0466, -0.0458,  0.0024,  0.0696,  0.0414, -0.0279,\n",
       "         -0.0705, -0.0537, -0.0027,  0.0765, -0.0384, -0.0015, -0.0854,  0.0746,\n",
       "         -0.0648,  0.0534,  0.0848,  0.0604], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0070, -0.1074,  0.0769, -0.0070,  0.0314, -0.0482,  0.0508, -0.1039,\n",
       "          -0.0884,  0.0641,  0.0390, -0.1009, -0.0797,  0.0306, -0.0282,  0.0164,\n",
       "          -0.1086, -0.0760, -0.0364, -0.0916,  0.0851, -0.0834, -0.0294, -0.0709,\n",
       "          -0.0573, -0.0062,  0.0034, -0.0799,  0.0203,  0.0197,  0.0009,  0.0105,\n",
       "          -0.0984,  0.0590,  0.0562,  0.1078,  0.0552, -0.0423, -0.1090, -0.0039,\n",
       "           0.0526,  0.0243, -0.0537, -0.0369, -0.0720, -0.0761, -0.0512, -0.0697,\n",
       "           0.0565,  0.1078,  0.0556,  0.0442,  0.0730,  0.0295, -0.0643,  0.0009,\n",
       "           0.0503, -0.0964, -0.0996, -0.0113, -0.0600,  0.0565, -0.0035, -0.0813,\n",
       "          -0.1073,  0.0155, -0.0538,  0.0258, -0.0458,  0.0698,  0.0942, -0.0554,\n",
       "           0.0794,  0.0268,  0.1082,  0.0801, -0.0648,  0.0828,  0.0101, -0.0086,\n",
       "           0.0044, -0.0874, -0.0480,  0.0405],\n",
       "         [-0.0071, -0.1013,  0.0182, -0.0688, -0.0640, -0.0970, -0.0666, -0.0669,\n",
       "           0.0641, -0.0477,  0.1008,  0.0654,  0.0258, -0.0115,  0.0022, -0.0448,\n",
       "          -0.0304, -0.0883, -0.0890,  0.0379,  0.0220, -0.0274, -0.0632,  0.0302,\n",
       "           0.1029, -0.0902, -0.0360,  0.0035,  0.0735, -0.0001, -0.0694,  0.0958,\n",
       "           0.0896,  0.0019,  0.0047,  0.0260, -0.0173,  0.0484,  0.0869,  0.0977,\n",
       "          -0.0235,  0.0287, -0.0705, -0.0847, -0.0132,  0.0555, -0.0408, -0.0673,\n",
       "          -0.0983,  0.0899,  0.0024,  0.0041,  0.0246,  0.0080,  0.0894, -0.0444,\n",
       "           0.0946,  0.0691,  0.0399,  0.0810, -0.0994, -0.1030, -0.0899,  0.0837,\n",
       "           0.0887,  0.0028,  0.0445,  0.0634,  0.0145, -0.0521,  0.0388, -0.0704,\n",
       "          -0.0598,  0.0532,  0.0511,  0.0330, -0.0935, -0.0241, -0.0357, -0.0492,\n",
       "          -0.0653, -0.0492,  0.0849, -0.0725],\n",
       "         [ 0.0346,  0.0452,  0.0923,  0.1005, -0.0618,  0.0084, -0.0690,  0.0256,\n",
       "          -0.0276,  0.0423,  0.0765, -0.0356,  0.0987,  0.0605,  0.0493, -0.1033,\n",
       "           0.0530, -0.0386, -0.0815,  0.0655,  0.0024,  0.1073, -0.0744,  0.0865,\n",
       "           0.0060,  0.0763,  0.0201, -0.0487,  0.0470,  0.0765,  0.0547, -0.0149,\n",
       "           0.0778,  0.0154,  0.0103, -0.0250,  0.0048, -0.0765, -0.0810,  0.0724,\n",
       "          -0.0631, -0.0060, -0.0990,  0.1053,  0.0469, -0.0144,  0.0306,  0.0535,\n",
       "          -0.1044,  0.0768,  0.0987, -0.0518,  0.1029, -0.0400,  0.0892, -0.0910,\n",
       "          -0.0766,  0.0018,  0.0136, -0.0753, -0.0825, -0.0465, -0.0282, -0.0310,\n",
       "          -0.0935,  0.0003,  0.0677, -0.0315,  0.0244, -0.0595, -0.0028, -0.0535,\n",
       "          -0.0825, -0.0957, -0.0933,  0.0490,  0.0293,  0.0898, -0.0143,  0.0606,\n",
       "           0.0705, -0.0645, -0.0773,  0.0080],\n",
       "         [ 0.0501,  0.0563, -0.0940, -0.0696, -0.0980,  0.0024,  0.0479, -0.0030,\n",
       "           0.0561, -0.1004,  0.0214, -0.0842,  0.0630,  0.0943, -0.0407, -0.1082,\n",
       "           0.0105,  0.0438,  0.0246, -0.0363,  0.0461, -0.0932, -0.0868,  0.0500,\n",
       "           0.0205, -0.0804,  0.0549, -0.1059,  0.0860, -0.1075, -0.0789, -0.0106,\n",
       "          -0.0658,  0.0841,  0.0512,  0.0225, -0.1054, -0.0755, -0.0373,  0.0899,\n",
       "           0.0753,  0.0204,  0.0577, -0.1050,  0.0379, -0.0105, -0.0585,  0.0686,\n",
       "           0.0936, -0.0261,  0.0841, -0.0014, -0.0311, -0.0767, -0.0086,  0.0189,\n",
       "           0.0530, -0.0364, -0.0036, -0.0365, -0.0260,  0.1042,  0.0414, -0.0217,\n",
       "          -0.0879,  0.0257,  0.0823,  0.0373, -0.0125, -0.0827, -0.1063,  0.0422,\n",
       "          -0.0011,  0.0845,  0.0580,  0.0524,  0.0625,  0.0051, -0.0041, -0.0063,\n",
       "          -0.0186,  0.0002,  0.0604,  0.0796],\n",
       "         [ 0.0824,  0.0428, -0.0244, -0.0917, -0.0509,  0.0281,  0.1065, -0.0875,\n",
       "          -0.0214,  0.0617,  0.0279, -0.0669, -0.0306,  0.0056, -0.0334,  0.0555,\n",
       "          -0.0803,  0.0690,  0.0823, -0.0986,  0.0882,  0.0604,  0.0021,  0.1038,\n",
       "          -0.0273,  0.1007, -0.0056, -0.0449,  0.0013,  0.0429,  0.0493,  0.0765,\n",
       "           0.0074, -0.0125, -0.0265,  0.0999, -0.0876, -0.0229,  0.0604,  0.0298,\n",
       "          -0.0313,  0.1020,  0.0525, -0.0424, -0.0342,  0.0340, -0.0380, -0.1046,\n",
       "          -0.0392, -0.0071, -0.0615, -0.0390,  0.0185,  0.1048,  0.0873,  0.0302,\n",
       "           0.0809,  0.0666,  0.0451, -0.0208,  0.0306,  0.0299, -0.0556,  0.0175,\n",
       "          -0.0841, -0.1061, -0.0506, -0.0582,  0.0405, -0.0609, -0.1084,  0.1081,\n",
       "           0.0027,  0.0457,  0.1045,  0.0683,  0.0360, -0.0770,  0.0378,  0.0655,\n",
       "           0.0811, -0.0364, -0.0097,  0.0789],\n",
       "         [-0.0602, -0.0364,  0.0788, -0.0235,  0.1028, -0.0273, -0.0697, -0.0338,\n",
       "          -0.0986, -0.0813,  0.0691,  0.1038, -0.0891,  0.0150,  0.0839, -0.0387,\n",
       "           0.0681,  0.0155,  0.0505,  0.0929, -0.1056,  0.0481, -0.0305, -0.0700,\n",
       "           0.0191,  0.0038,  0.0031,  0.1046, -0.0269, -0.0586,  0.0852,  0.0155,\n",
       "          -0.0227, -0.0977, -0.0601, -0.0301, -0.0615,  0.0057,  0.0576, -0.1014,\n",
       "          -0.0064, -0.0598, -0.0233, -0.1064, -0.1050, -0.0977,  0.0694, -0.0809,\n",
       "           0.1046,  0.0035, -0.0637, -0.0553, -0.0441, -0.0194,  0.0625, -0.0653,\n",
       "          -0.0972, -0.0150, -0.0208, -0.0619, -0.0297,  0.0552,  0.0074, -0.0694,\n",
       "           0.0662,  0.0015, -0.0919,  0.0380, -0.0592,  0.0301,  0.0529,  0.0850,\n",
       "           0.0548, -0.0978, -0.0907, -0.0561, -0.1086,  0.0589,  0.0223,  0.0138,\n",
       "           0.1090, -0.0779,  0.0802, -0.0806],\n",
       "         [ 0.0676, -0.0191, -0.0482, -0.0147, -0.1044,  0.0041,  0.0859, -0.0528,\n",
       "           0.0591,  0.0976, -0.0124,  0.1086, -0.0518,  0.0938, -0.0393, -0.0181,\n",
       "           0.0500,  0.0235, -0.0492,  0.0123, -0.0611, -0.0645, -0.0566, -0.1076,\n",
       "          -0.0920, -0.0891,  0.0310, -0.0416, -0.0561, -0.0826, -0.0786, -0.0078,\n",
       "           0.0089, -0.0298, -0.0873, -0.0205,  0.0175,  0.0617,  0.0781, -0.1013,\n",
       "          -0.0008,  0.0743, -0.0705, -0.0729, -0.0348,  0.0027,  0.1078,  0.0916,\n",
       "          -0.0100, -0.0856, -0.0948,  0.1017, -0.1028, -0.0637, -0.0884, -0.0532,\n",
       "           0.0408, -0.0522,  0.0271,  0.0452,  0.0703,  0.0558,  0.0591,  0.0500,\n",
       "           0.0290, -0.0095, -0.0877, -0.1060,  0.0547, -0.0892,  0.0284,  0.0359,\n",
       "          -0.1082,  0.0164,  0.0673, -0.0564, -0.0426,  0.0844, -0.0018, -0.0307,\n",
       "          -0.0961, -0.0712,  0.0585, -0.0505],\n",
       "         [-0.0717,  0.0335, -0.0580,  0.0508, -0.0885,  0.0936,  0.0867, -0.0335,\n",
       "          -0.0820, -0.0164,  0.0868,  0.0631,  0.0841, -0.1056, -0.0553,  0.0421,\n",
       "           0.0707,  0.1008, -0.0299,  0.0055, -0.0779,  0.1012,  0.0011, -0.1020,\n",
       "           0.0411,  0.0309, -0.0502,  0.0179,  0.1087, -0.0324,  0.0857, -0.0784,\n",
       "           0.0934, -0.0476,  0.0035, -0.1065, -0.0458, -0.0081, -0.0119,  0.0136,\n",
       "          -0.0213, -0.1076,  0.0125, -0.0139, -0.0006, -0.1013,  0.0114,  0.0628,\n",
       "           0.0007, -0.0633, -0.0364, -0.0134, -0.0278,  0.0070,  0.0888, -0.0146,\n",
       "           0.0610, -0.0731, -0.0423,  0.0210, -0.0847, -0.0675, -0.0524, -0.1087,\n",
       "          -0.0482,  0.0457, -0.0409, -0.0332, -0.0333,  0.0856, -0.0877,  0.0729,\n",
       "          -0.0310,  0.0123, -0.0079, -0.0210, -0.0593,  0.0308,  0.0112, -0.0795,\n",
       "          -0.0806,  0.0177, -0.0033,  0.0771],\n",
       "         [ 0.0920, -0.0712,  0.0155, -0.0711,  0.0404,  0.0352, -0.0737,  0.0610,\n",
       "          -0.0513,  0.0612, -0.0488, -0.0150,  0.0037,  0.0060,  0.0597,  0.0960,\n",
       "          -0.0692, -0.0031, -0.0275,  0.0008, -0.0657, -0.0895,  0.0215,  0.0258,\n",
       "          -0.0459,  0.0787, -0.0929, -0.0972, -0.0398,  0.0136,  0.0685,  0.0360,\n",
       "           0.0911,  0.0594,  0.0735, -0.0731, -0.0236,  0.0737, -0.0207,  0.1027,\n",
       "           0.0483,  0.0066, -0.0620,  0.1088, -0.0148,  0.0565, -0.0171,  0.0367,\n",
       "           0.0804,  0.1050,  0.0336, -0.0251,  0.0487, -0.0205, -0.0447,  0.0541,\n",
       "          -0.0234,  0.0058, -0.1064, -0.0496, -0.0140,  0.0096, -0.0400,  0.0860,\n",
       "           0.1008, -0.0576,  0.0777, -0.0363,  0.0033, -0.0229, -0.0147,  0.0207,\n",
       "           0.0714,  0.0137, -0.0476, -0.0012, -0.0529,  0.0325, -0.0544,  0.0199,\n",
       "          -0.0970, -0.0219,  0.1066, -0.1054],\n",
       "         [-0.0843, -0.0732, -0.0612, -0.0835,  0.0969, -0.0306,  0.0889,  0.0040,\n",
       "           0.0401, -0.0486, -0.0497,  0.0801, -0.0476,  0.0539, -0.0052,  0.0430,\n",
       "          -0.0431, -0.0388,  0.0281, -0.0581, -0.0176, -0.0949,  0.0239, -0.0139,\n",
       "          -0.0203, -0.0547, -0.0879,  0.0764, -0.0355,  0.0204, -0.0809,  0.0571,\n",
       "           0.0250,  0.0037, -0.0740,  0.0202, -0.0435,  0.0973,  0.0843, -0.1017,\n",
       "           0.0191, -0.0060,  0.0782,  0.0949, -0.0600,  0.1069, -0.0056,  0.0423,\n",
       "          -0.1008, -0.0155, -0.0719, -0.0450, -0.0452,  0.0865,  0.0932,  0.0969,\n",
       "           0.0827,  0.0282, -0.0741, -0.0753,  0.0919,  0.0454,  0.0067,  0.0463,\n",
       "           0.1076, -0.0676, -0.0679, -0.0448,  0.0846,  0.1018,  0.0049, -0.0950,\n",
       "           0.0021, -0.0584, -0.0241, -0.0764,  0.0622,  0.0283, -0.0219, -0.0868,\n",
       "          -0.0417,  0.0027, -0.0621, -0.0484]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0231, -0.0859,  0.0522,  0.0993, -0.0598, -0.0150, -0.0050, -0.0828,\n",
       "          0.0387, -0.0056], requires_grad=True)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 可以在forward 函数中使用tensor 中操作\n",
    "params = list(net.parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0].size() #conv1's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1009, -0.1753,  0.0722,  0.0634,  0.0077, -0.0767, -0.0371, -0.1386,\n",
       "         -0.0131, -0.0438]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = Variable(torch.randn(1,1,32,32))\n",
    "out = net.forward(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad() ## 对所有的参数梯度缓冲去进行归零\n",
    "out.backward(torch.randn(1,10)) # 使用随机的梯度进行反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn 只接受小批量数据\n",
    "整个torch.nn包只接受那种小批量样本的数据，而非单个样本，例如nn.Conv2d\n",
    "能够结构一个四维的TensornSamples * nChannels * Height * Width\n",
    "如果你拿到是当个样本那么使用input.unsqueeze(0)来加一个维度就可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.9734, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## nn.MSELoss计算输入和目标之间的均方差\n",
    "output = net.forward(input)\n",
    "target = Variable(torch.arange(1,11))\n",
    "criterion =  nn.MSELoss()\n",
    "loss = criterion(output,target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 现在当调用loss.backward() 之后来看看conv1's在进行反馈之后的偏置梯度如何\n",
    "net.zero_grad()\n",
    "net.conv1.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新网络的权重\n",
    "## 最简单方法是随机梯度下降法(SGD)\n",
    "weight = weight - learning_rate* gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate  = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected dtype Float but got dtype Long (validate_dtype at ../aten/src/ATen/native/TensorIterator.cpp:143)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x1282fef47 in libc10.dylib)\nframe #1: at::TensorIterator::compute_types() + 4335 (0x11b39730f in libtorch_cpu.dylib)\nframe #2: at::TensorIterator::build() + 690 (0x11b39b6c2 in libtorch_cpu.dylib)\nframe #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 520 (0x11b1d1f08 in libtorch_cpu.dylib)\nframe #4: at::CPUType::mse_loss_backward_out_grad_input(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 247 (0x11b64ad07 in libtorch_cpu.dylib)\nframe #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 193 (0x11b1d1c01 in libtorch_cpu.dylib)\nframe #6: at::CPUType::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 234 (0x11b64b05a in libtorch_cpu.dylib)\nframe #7: c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, at::Tensor const&, long long> >, at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 27 (0x11b68557b in libtorch_cpu.dylib)\nframe #8: torch::autograd::VariableType::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 2608 (0x11d460810 in libtorch_cpu.dylib)\nframe #9: c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, at::Tensor const&, long long> >, at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 27 (0x11b68557b in libtorch_cpu.dylib)\nframe #10: at::Tensor c10::OperatorHandle::callUnboxed<at::Tensor, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long>(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) const + 294 (0x11c5826c6 in libtorch_cpu.dylib)\nframe #11: torch::autograd::generated::MseLossBackward::apply(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&) + 311 (0x11d1b4c77 in libtorch_cpu.dylib)\nframe #12: torch::autograd::Node::operator()(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&) + 658 (0x11d97c742 in libtorch_cpu.dylib)\nframe #13: torch::autograd::Engine::evaluate_function(std::__1::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 1408 (0x11d972f10 in libtorch_cpu.dylib)\nframe #14: torch::autograd::Engine::thread_main(std::__1::shared_ptr<torch::autograd::GraphTask> const&, bool) + 497 (0x11d972121 in libtorch_cpu.dylib)\nframe #15: torch::autograd::Engine::thread_init(int) + 152 (0x11d971eb8 in libtorch_cpu.dylib)\nframe #16: torch::autograd::python::PythonEngine::thread_init(int) + 52 (0x11a4a6dc4 in libtorch_python.dylib)\nframe #17: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (torch::autograd::Engine::*)(int), torch::autograd::Engine*, int> >(void*) + 66 (0x11d9814a2 in libtorch_cpu.dylib)\nframe #18: _pthread_start + 148 (0x7fff72612109 in libsystem_pthread.dylib)\nframe #19: thread_start + 15 (0x7fff7260db8b in libsystem_pthread.dylib)\n",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"<ipython-input-156-c330e45c3419>\"\u001b[0m, line \u001b[1;32m8\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    loss.backward()\n",
      "  File \u001b[1;32m\"/Users/mensyne/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\"\u001b[0m, line \u001b[1;32m198\u001b[0m, in \u001b[1;35mbackward\u001b[0m\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/mensyne/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\"\u001b[0;36m, line \u001b[0;32m100\u001b[0;36m, in \u001b[0;35mbackward\u001b[0;36m\u001b[0m\n\u001b[0;31m    allow_unreachable=True)  # allow_unreachable flag\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m\u001b[0;31m:\u001b[0m expected dtype Float but got dtype Long (validate_dtype at ../aten/src/ATen/native/TensorIterator.cpp:143)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x1282fef47 in libc10.dylib)\nframe #1: at::TensorIterator::compute_types() + 4335 (0x11b39730f in libtorch_cpu.dylib)\nframe #2: at::TensorIterator::build() + 690 (0x11b39b6c2 in libtorch_cpu.dylib)\nframe #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 520 (0x11b1d1f08 in libtorch_cpu.dylib)\nframe #4: at::CPUType::mse_loss_backward_out_grad_input(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 247 (0x11b64ad07 in libtorch_cpu.dylib)\nframe #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 193 (0x11b1d1c01 in libtorch_cpu.dylib)\nframe #6: at::CPUType::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 234 (0x11b64b05a in libtorch_cpu.dylib)\nframe #7: c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, at::Tensor const&, long long> >, at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 27 (0x11b68557b in libtorch_cpu.dylib)\nframe #8: torch::autograd::VariableType::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 2608 (0x11d460810 in libtorch_cpu.dylib)\nframe #9: c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, at::Tensor const&, long long> >, at::Tensor (at::Tensor const&, at::Tensor const&, at::Tensor const&, long long)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) + 27 (0x11b68557b in libtorch_cpu.dylib)\nframe #10: at::Tensor c10::OperatorHandle::callUnboxed<at::Tensor, at::Tensor const&, at::Tensor const&, at::Tensor const&, long long>(at::Tensor const&, at::Tensor const&, at::Tensor const&, long long) const + 294 (0x11c5826c6 in libtorch_cpu.dylib)\nframe #11: torch::autograd::generated::MseLossBackward::apply(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&) + 311 (0x11d1b4c77 in libtorch_cpu.dylib)\nframe #12: torch::autograd::Node::operator()(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&) + 658 (0x11d97c742 in libtorch_cpu.dylib)\nframe #13: torch::autograd::Engine::evaluate_function(std::__1::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 1408 (0x11d972f10 in libtorch_cpu.dylib)\nframe #14: torch::autograd::Engine::thread_main(std::__1::shared_ptr<torch::autograd::GraphTask> const&, bool) + 497 (0x11d972121 in libtorch_cpu.dylib)\nframe #15: torch::autograd::Engine::thread_init(int) + 152 (0x11d971eb8 in libtorch_cpu.dylib)\nframe #16: torch::autograd::python::PythonEngine::thread_init(int) + 52 (0x11a4a6dc4 in libtorch_python.dylib)\nframe #17: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (torch::autograd::Engine::*)(int), torch::autograd::Engine*, int> >(void*) + 66 (0x11d9814a2 in libtorch_cpu.dylib)\nframe #18: _pthread_start + 148 (0x7fff72612109 in libsystem_pthread.dylib)\nframe #19: thread_start + 15 (0x7fff7260db8b in libsystem_pthread.dylib)\n\n"
     ]
    }
   ],
   "source": [
    "## 当我们想用不同种类的更新权重方法 可以考虑torch.optim 这个功能\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.001)\n",
    "# in your training looop\n",
    "optimizer.zero_grad() ## zero the gradient buffers\n",
    "output = net.forward(input)\n",
    "loss = criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
