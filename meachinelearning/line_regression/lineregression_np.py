import numpy as npimport  matplotlib.pyplot as pltdef linearRegression(alpha=0.01,num_iters=400):    print("加载数据....\n")    data = loadtxtAndcsv_data('data.txt',',',np.float64)    X = data[:,0:-1]    y = data[:,-1]    m = len(y)  # y的行数    col= data.shape[1] # data的列数    X = np.stack((np.ones(m,1)),X)  # 在X前面加上一列1    print("执行梯度下降算法.....")    theta = np.zeros((col,1))    y = y.reshape(-1,1) # 将行向量转化为列    theta,J_history = gradientDescent(X,y,theta,alpha,num_iters)    plotJ(J_history,num_iters)    return mu,sigma,theta  #返回均值mu，标准差sigma,和学习的结果theta# 加载txt和csv 文件def loadtxtAndcsv_data(filename,split,dataType):    return np.loadtxt(filename,delimiter= split,dtype=dataType)# 加载npy 文件def loadnpy_data(filename):    return np.load(filename)# 归一化featuredef featureNormalize(X):    X_norm = np.array(X)  # 将X转换为numpy数组对象    # 定义所需的变量    mu = np.zeros((1,X.shape[1]))    sigma = np.zeros((1,X.shape[1]))    mu = np.mean(X_norm,0) # 求每一列的平均值    sigma = np.std(X_norm,0)    for i in range(X.shape[1]): # 遍历列        X_norm[:,i] = (X_norm[:,i] - mu[i]) /sigma[i]  # 进行归一化处理    return X_norm,mu,sigma# 画二维图def plot_X1_X2(X):    plt.scatter(X[:,0],X[:,1])    plt.show()# 梯度下降算法def gradientDescent(X,y,theta,alpha,num_iters):    m = len(y)    n = len(theta)    temp = np.matrix(np.zeros((n,num_iters)))    J_history = np.zeros((num_iters,1)) # 记录每次迭代计算的代价函数    for i in range(num_iters):        h = np.dot(X,theta)        temp[:,i] = theta - ((alpha/m)*(np.dot(np.transpose(X),h-y))) # 梯度的计算        theta = temp[:,i]        J_history[i] = computerCost(X,y,theta) # 调用计算代价函数    return theta,J_history# 计算代价函数def computerCost(X,y,theta):    m = len(y)    J = 0    J = (np.transpose(X*theta-y))*(X*theta-y)/(2*m)  #计算代价J    return J# 画出每次迭代变化图def plotJ(J_history,num_iters):    x = np.arange(1,num_iters+1)    plt.plot(x,J_history)    plt.xlabel('迭代次数')    plt.ylabel('代价值')    plt.title('代价随迭代次数的变化')    plt.show()# 测试linearRression函数def testLinearRegression():    mu,sigma,theta = linearRegression(0.01,400)# 测试学习效果def predict(mu,sigma,theta):    result = 0    # 注意归一化    predict = np.array([1650,3])    norm_predict = (predict-mu)/sigma    final_predict = np.hstack((np.ones((1)),norm_predict))    result = np.dot(final_predict,theta) # 预测结果    return resultif __name__ == '__main__':    testLinearRegression()